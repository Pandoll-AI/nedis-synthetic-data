# NEDIS í•©ì„± ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œ: ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” NEDIS (National Emergency Department Information System) í•©ì„± ë°ì´í„° ìƒì„± ì‹œìŠ¤í…œì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì†ŒìŠ¤ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë¶„ì„í•˜ê³ , ì„±ëŠ¥ íŠ¹ì„±ê³¼ ê°œì¸ì •ë³´ë³´í˜¸ ìœ„í—˜ì„±ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë° ì „ì²´ Flow

### 1. ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê°œìš”

NEDIS ì‹œìŠ¤í…œì€ **3ë‹¨ê³„ ë¶„ë¦¬í˜• ë²¡í„°í™” ì•„í‚¤í…ì²˜**ë¥¼ ì±„íƒí•˜ì—¬ 50ë°° ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:

```
[Phase 1: ë™ì  íŒ¨í„´ ë¶„ì„] â†’ [Phase 2: ë²¡í„°í™” í•©ì„±] â†’ [Phase 3: í†µê³„ì  ê²€ì¦]
     (EDA ë‹¨ê³„)           (ë°ì´í„° ìƒì„±)        (í’ˆì§ˆ ë³´ì¦)
```

### 2. Phase 1: ë™ì  íŒ¨í„´ ë¶„ì„ ë‹¨ê³„ (EDA)

#### 2.1 í•µì‹¬ ëª¨ë“ˆ: `PatternAnalyzer` 
**ìœ„ì¹˜**: `src/analysis/pattern_analyzer.py`

**ì£¼ìš” ê¸°ëŠ¥**:
```python
def analyze_all_patterns() -> Dict[str, Any]:
    # 1. ë°ì´í„° í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
    data_hash = get_data_hash(db_manager, "nedis_original.nedis2017")
    
    # 2. 5ê°€ì§€ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
    patterns = {
        "hospital_allocation": analyze_hospital_allocation_patterns(),
        "ktas_distributions": analyze_ktas_distributions(),  
        "regional_patterns": analyze_regional_patterns(),
        "demographic_patterns": analyze_demographic_patterns(),
        "temporal_patterns": analyze_temporal_patterns()
    }
    
    # 3. ìºì‹œ ì €ì¥/ë¡œë“œ ê´€ë¦¬
    # 4. ë©”íƒ€ë°ì´í„° ìƒì„±
    return patterns
```

#### 2.2 ê³„ì¸µì  ëŒ€ì•ˆ ì‹œìŠ¤í…œ (í•µì‹¬ í˜ì‹ )

**KTAS ë¶„í¬ ì¡°íšŒ ì˜ˆì‹œ**:
```python
def get_hierarchical_ktas_distribution(region_code: str, hospital_type: str) -> Dict[str, float]:
    """
    4ë‹¨ê³„ ê³„ì¸µì  ëŒ€ì•ˆ:
    1ë‹¨ê³„: ì†Œë¶„ë¥˜(4ìë¦¬ì§€ì—­ì½”ë“œ) + ë³‘ì›ìœ í˜• â†’ detailed_patterns
    2ë‹¨ê³„: ëŒ€ë¶„ë¥˜(ì²«2ìë¦¬) + ë³‘ì›ìœ í˜• â†’ major_patterns  
    3ë‹¨ê³„: ì „êµ­ + ë³‘ì›ìœ í˜• â†’ national_patterns
    4ë‹¨ê³„: ì „ì²´ í‰ê·  â†’ overall_pattern (ìµœì¢… ëŒ€ì•ˆ)
    """
```

**ê³„ì¸µì  ëŒ€ì•ˆì˜ ì¥ì **:
- **ë°ì´í„° í¬ì†Œì„± í•´ê²°**: ì‘ì€ ì§€ì—­ì˜ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°
- **í†µê³„ì  ì•ˆì •ì„±**: ì¶©ë¶„í•œ ìƒ˜í”Œ í¬ê¸° ë³´ì¥
- **í™•ì¥ì„±**: ì „êµ­ ê·œëª¨ë¡œ í™•ì¥ ì‹œì—ë„ ì•ˆì •ì  ë™ì‘

#### 2.3 ìºì‹± ì‹œìŠ¤í…œ

**í•´ì‹œ ê¸°ë°˜ ë³€ê²½ ê°ì§€**:
```python
def get_data_hash(db_manager, table_name):
    # í…Œì´ë¸” í–‰ìˆ˜ + ìƒ˜í”Œ ë°ì´í„°ë¡œ MD5 í•´ì‹œ ê³„ì‚°
    count_result = db_manager.fetch_dataframe(f"SELECT COUNT(*) FROM {table_name}")
    sample_data = db_manager.fetch_dataframe(f"SELECT * FROM {table_name} ORDER BY RANDOM() LIMIT 1000")
    hash_input = f"{table_name}_{count_result}_{sample_data.to_string()}"
    return hashlib.md5(hash_input.encode()).hexdigest()
```

**ì €ì¥ ë°©ì‹**:
- **ë¶„ì„ ê²°ê³¼**: Pickle í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”
- **ë©”íƒ€ë°ì´í„°**: JSON í˜•ì‹ìœ¼ë¡œ ì¸ë±ì‹±
- **ì„ íƒì  ë¬´íš¨í™”**: íŠ¹ì • ë¶„ì„ë§Œ ì¬ìˆ˜í–‰ ê°€ëŠ¥

### 3. Phase 2: ë²¡í„°í™” í•©ì„± ë°ì´í„° ìƒì„±

#### 3.1 ë©”ì¸ íŒŒì´í”„ë¼ì¸: `run_vectorized_pipeline.py`

**4-Stage ì‹¤í–‰ ìˆœì„œ**:
```python
def run_vectorized_pipeline(args):
    # Stage 1: ë²¡í„°í™” í™˜ì ìƒì„± (ë‚ ì§œ ì—†ìŒ)
    patients_df = patient_generator.generate_all_patients(generation_config)
    
    # Stage 2: ì‹œê°„ íŒ¨í„´ í• ë‹¹  
    patients_with_dates = temporal_assigner.assign_temporal_patterns(
        patients_df, temporal_config
    )
    
    # Stage 3: ë³‘ì› ìš©ëŸ‰ ì œì•½ ì ìš©
    final_patients = capacity_processor.apply_capacity_constraints(
        patients_with_dates, capacity_config
    )
    
    # Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    save_to_database(final_patients, db_manager, args)
```

#### 3.2 Stage 1: ë²¡í„°í™” í™˜ì ìƒì„±

**ìœ„ì¹˜**: `src/vectorized/patient_generator.py`

**4ë‹¨ê³„ ë²¡í„°í™” í™˜ì ìƒì„±**:
```python
def _generate_patients_vectorized(total_records):
    # Stage 1: ì¸êµ¬í†µê³„ ë²¡í„° ìƒì„±
    demographics_df = _generate_demographics_vectorized(total_records)
    
    # Stage 2: ì´ˆê¸° ë³‘ì› í• ë‹¹ (ì§€ì—­ ê¸°ë°˜, ì¤‘ë ¥ëª¨ë¸ ì œê±°)
    hospital_assignments = _generate_hospital_assignments_vectorized(demographics_df)
    
    # Stage 3: ë…ë¦½ì  ì„ìƒ ì†ì„± ìƒì„± (ì™„ì „ ë²¡í„°í™”)
    clinical_attrs = _generate_independent_clinical_attributes(demographics_df)
    
    # Stage 4: ì¡°ê±´ë¶€ ì„ìƒ ì†ì„± ìƒì„± (Semi-ë²¡í„°í™”)
    # KTAS â†’ ì¹˜ë£Œê²°ê³¼ ì˜ì¡´ì„±ë§Œ ì¼ê´„ì²˜ë¦¬
    conditional_attrs = _generate_conditional_clinical_attributes(
        demographics_df, clinical_attrs
    )
    
    return pd.concat([demographics_df, clinical_attrs, conditional_attrs], axis=1)
```

**ë™ì  íŒ¨í„´ í™œìš© ë°©ì‹**:
- **íŒ¨í„´ ë¡œë“œ**: `self.pattern_analyzer.analyze_all_patterns()` í˜¸ì¶œ
- **ê³„ì¸µì  ì¡°íšŒ**: `get_hierarchical_ktas_distribution(region_code, hospital_type)`
- **ì§€ì—­ ê¸°ë°˜ í• ë‹¹**: ì‹¤ì œ í™˜ì ìœ ë™ íŒ¨í„´ ì‚¬ìš© (ì¤‘ë ¥ëª¨ë¸ ëŒ€ì‹ )
- **ë°±ì—… ë¶„í¬**: ë™ì  ë¶„ì„ ì‹¤íŒ¨ì‹œ `_cached_distributions` ì‚¬ìš©

#### 3.3 Stage 2: ì‹œê°„ íŒ¨í„´ í• ë‹¹

**ìœ„ì¹˜**: `src/vectorized/temporal_assigner.py`

**ì‹œê°„ í• ë‹¹ ì›Œí¬í”Œë¡œìš°**:
```python
def assign_temporal_patterns(patients_df, temporal_config):
    # 1. ë™ì  ì‹œê°„ íŒ¨í„´ ë¡œë“œ
    self._load_temporal_patterns(temporal_config.year)
    
    # 2. NHPP(Non-Homogeneous Poisson Process) ê¸°ë°˜ ì¼ë³„ ë³¼ë¥¨ ê³„ì‚°
    daily_volumes = self._calculate_daily_volumes(temporal_config)
    
    # 3. ë²¡í„°í™”ëœ ë‚ ì§œ í• ë‹¹
    result_df = self._assign_dates_vectorized(patients_df, daily_volumes)
    
    # 4. ì‹œê°„ í• ë‹¹ (ì‹œê°„ë³„ í•´ìƒë„ì¼ ê²½ìš°)
    if temporal_config.time_resolution == 'hourly':
        result_df = self._assign_times_vectorized(result_df, temporal_config)
    
    return result_df
```

**ë³´ì¡´ë˜ëŠ” ì‹œê°„ íŒ¨í„´ë“¤**:
- **ê³„ì ˆì„± íŒ¨í„´**: ì›”ë³„ ë‚´ì› íŒ¨í„´ ë³´ì¡´
- **ì£¼ê°„ íŒ¨í„´**: ì£¼ë§/ì£¼ì¤‘ ì°¨ì´ ë°˜ì˜
- **ê³µíœ´ì¼ íš¨ê³¼**: 2017ë…„ í•œêµ­ ê³µíœ´ì¼ íŒ¨í„´ ë°˜ì˜
- **ì‹œê°„ëŒ€ë³„ íŒ¨í„´**: 24ì‹œê°„ ë‚´ì› ë¶„í¬ ë³´ì¡´

#### 3.4 Stage 3: ë³‘ì› ìš©ëŸ‰ ì œì•½ ì ìš©

**ìœ„ì¹˜**: `src/vectorized/capacity_processor.py`

**ìš©ëŸ‰ ì œì•½ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°**:
```python
def apply_capacity_constraints(patients_df, capacity_config):
    # 1. ìš©ëŸ‰ ì°¸ì¡° ë°ì´í„° ë¡œë“œ
    self._load_capacity_reference_data()
    
    # 2. ë™ì  ìš©ëŸ‰ ì œí•œ ê³„ì‚° (ì£¼ë§/ê³µíœ´ì¼ ì¡°ì •)
    daily_capacity_limits = self._calculate_dynamic_capacity_limits(capacity_config)
    
    # 3. í˜„ì¬ ë³‘ì›ë³„ ë¶€í•˜ ê³„ì‚°
    current_loads = self._calculate_current_loads(patients_df)
    
    # 4. Overflow ê°ì§€ ë° ì¬í• ë‹¹
    result_df = self._redistribute_overflow_patients(
        patients_df, current_loads, daily_capacity_limits, capacity_config
    )
    
    return result_df
```

**ìš©ëŸ‰ ì¡°ì • ìš”ì†Œë“¤**:
- **ê¸°ë³¸ ìš©ëŸ‰**: `daily_capacity_mean` ê¸°ì¤€
- **ì£¼ë§ ì¡°ì •**: 0.8ë°° (ê¸°ë³¸ê°’)
- **ê³µíœ´ì¼ ì¡°ì •**: 0.7ë°° (ê¸°ë³¸ê°’)  
- **ì•ˆì „ ì—¬ìœ **: 1.2ë°° (ê¸°ë³¸ê°’)
- **ì¬í• ë‹¹ ë°©ë²•**: nearest_available, random_available, second_choice_probability

### 4. Phase 3: í†µê³„ì  ê²€ì¦ ë‹¨ê³„

#### 4.1 ê²€ì¦ ëª¨ë“ˆ: `StatisticalValidator`
**ìœ„ì¹˜**: `src/validation/statistical_validator.py`

**ê²€ì¦ ëŒ€ìƒ ë³€ìˆ˜ë“¤**:
```python
# ì—°ì†í˜• ë³€ìˆ˜ (Kolmogorov-Smirnov ê²€ì •)
continuous_variables = [
    'vst_sbp', 'vst_dbp', 'vst_per_pu', 'vst_per_br', 'vst_bdht', 'vst_oxy'
]

# ë²”ì£¼í˜• ë³€ìˆ˜ (Chi-square ê²€ì •)  
categorical_variables = [
    'pat_age_gr', 'pat_sex', 'pat_do_cd', 'ktas_fstu', 'emtrt_rust', 
    'vst_meth', 'msypt', 'main_trt_p'
]
```

**ê²€ì¦ ë°©ë²•ë“¤**:
- **Kolmogorov-Smirnov ê²€ì •**: ì—°ì†í˜• ë³€ìˆ˜ ë¶„í¬ ìœ ì‚¬ì„±
- **Chi-square ê²€ì •**: ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ìœ ì‚¬ì„±
- **ìƒê´€ê´€ê³„ ë¶„ì„**: Pearson/Spearman ìƒê´€ê³„ìˆ˜ ë¹„êµ
- **Wasserstein distance**: Earth Mover's Distance
- **ë¶„í¬ í˜•íƒœ**: Quantile-Quantile plot ë¶„ì„

---

## âš¡ ì„±ëŠ¥ íŠ¹ì„± ë¶„ì„

### 1. ë²¡í„°í™”ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ

**ì„±ëŠ¥ ë¹„êµ**:
- **ì´ì „ ìˆœì°¨ ë°©ì‹**: ~300ì´ˆ (322K ë ˆì½”ë“œ)
- **ìƒˆë¡œìš´ ë²¡í„°í™” ë°©ì‹**: ~7ì´ˆ (322K ë ˆì½”ë“œ)
- **ì„±ëŠ¥ í–¥ìƒ**: **ì•½ 50ë°° ê°œì„ **

### 2. ì„±ëŠ¥ í–¥ìƒ í•µì‹¬ ìš”ì¸ë“¤

#### 2.1 ë‚ ì§œ ë¶„ë¦¬ ì „ëµ
```python
# ê¸°ì¡´: ë‚ ì§œë³„ ìˆœì°¨ ìƒì„±
for date in date_range:
    daily_patients = generate_patients_for_date(date)  # 365ë²ˆ ë°˜ë³µ

# ì‹ ê·œ: ì™„ì „ ë¶„ë¦¬
all_patients = generate_all_patients()  # 1íšŒ ìƒì„±
assign_dates_to_patients(all_patients)  # 1íšŒ í• ë‹¹
```

#### 2.2 ì™„ì „ ë²¡í„°í™”
- **NumPy ë°°ì—´ ì—°ì‚°**: ë°˜ë³µë¬¸ ëŒ€ì‹  ë²¡í„° ì—°ì‚° í™œìš©
- **Pandas ë²¡í„°í™”**: í–‰ë³„ ì²˜ë¦¬ ëŒ€ì‹  ì»¬ëŸ¼ ë‹¨ìœ„ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì²­í¬ë³„ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì§€ì›

#### 2.3 ë™ì  ìºì‹±
```python
# íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ìºì‹±
if cached_analysis_exists(data_hash):
    patterns = load_cached_patterns(data_hash)  # ì¦‰ì‹œ ë¡œë“œ
else:
    patterns = analyze_patterns_from_scratch()  # 1íšŒ ë¶„ì„ í›„ ìºì‹±
```

#### 2.4 Semi-ë²¡í„°í™” ì „ëµ
- **ë…ë¦½ ì†ì„±**: ì™„ì „ ë²¡í„°í™” (ë‚˜ì´, ì„±ë³„, ì§€ì—­ ë“±)
- **ì˜ì¡´ ì†ì„±**: ì¼ê´„ ì²˜ë¦¬ (KTAS â†’ ì¹˜ë£Œê²°ê³¼)

### 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

**ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬**:
```python
def _generate_patients_chunked(gen_config):
    chunks = []
    remaining = gen_config.total_records
    
    while remaining > 0:
        chunk_size = min(gen_config.batch_size, remaining)
        chunk_df = self._generate_patients_vectorized(chunk_size)
        chunks.append(chunk_df)
        remaining -= chunk_size
    
    return pd.concat(chunks, ignore_index=True)
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
- **ê¸°ë³¸ ì„¤ì •**: 50,000 ë ˆì½”ë“œ/ì²­í¬
- **ìµœëŒ€ ë©”ëª¨ë¦¬**: ~2GB (322K ë ˆì½”ë“œ ê¸°ì¤€)
- **í™•ì¥ì„±**: ìˆ˜ë°±ë§Œ ë ˆì½”ë“œê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥

---

## ğŸš¨ ê°œì¸ì •ë³´ë³´í˜¸ ìœ„í—˜ì„± ë¶„ì„

### 1. ë‹¨ê³„ë³„ ì¬ì‹ë³„ ìœ„í—˜ì„± í‰ê°€

#### 1.1 Stage 1: ë²¡í„°í™” í™˜ì ìƒì„± ë‹¨ê³„

**ğŸ”´ ë†’ì€ ìœ„í—˜ ìš”ì†Œë“¤**:

**ì§€ì—­-ë³‘ì› í• ë‹¹ íŒ¨í„´ ìœ„í—˜**:
```python
# src/vectorized/patient_generator.py:236-283
allocation_data = db.fetch_dataframe("""
    SELECT pat_do_cd, emorg_cd, COUNT(*) as visit_count,
           COUNT(*) * 1.0 / SUM(COUNT(*)) OVER(PARTITION BY pat_do_cd) as region_probability
    FROM nedis_original.nedis2017
""")
```
**ìœ„í—˜ì„±**: 4ìë¦¬ ì§€ì—­ì½”ë“œ + íŠ¹ì • ë³‘ì› ì¡°í•©ì´ **ë§¤ìš° êµ¬ì²´ì ì¸ ì§€ë¦¬ì  ì‹ë³„ì** ì—­í• 

**ê³„ì¸µì  KTAS ë¶„í¬ì˜ ê³¼ë„í•œ ì„¸ë¶„í™”**:
```python
# src/analysis/pattern_analyzer.py:288-326  
detailed_key = f"{region_code}_{hospital_type}"  # ì˜ˆ: "1101_large"
```
**ìœ„í—˜ì„±**: ì†Œë¶„ë¥˜ ì§€ì—­ + ë³‘ì›ê·œëª¨ ì¡°í•©ì´ **í¬ê·€ íŒ¨í„´ ìƒì„± ê°€ëŠ¥**

#### 1.2 Stage 2: ì‹œê°„ íŒ¨í„´ í• ë‹¹ ë‹¨ê³„

**ğŸ”´ ë†’ì€ ìœ„í—˜ ìš”ì†Œë“¤**:

**ì •ë°€í•œ ì‹œê°„ ì •ë³´**:
```python
# src/vectorized/temporal_assigner.py:74-79
if temporal_config.time_resolution == 'hourly':
    result_df = _assign_times_vectorized(result_df, temporal_config)
```
**ìœ„í—˜ì„±**: `vst_dt` + `vst_tm` ì¡°í•©ì´ **ì‹œê°„ì  ì§€ë¬¸(temporal fingerprint)** ìƒì„±

**ê³µíœ´ì¼ íŒ¨í„´ ë³´ì¡´**:
```python
# config/generation_params.yaml:18-34
holidays_2017: ["20170101", "20170127", "20170128", ...]
```
**ìœ„í—˜ì„±**: ê³µíœ´ì¼ ë‚´ì› íŒ¨í„´ì´ **í–‰ë™ì  íŠ¹ì´ì„±** ë…¸ì¶œ

#### 1.3 Stage 3: ë³‘ì› ìš©ëŸ‰ ì œì•½ ì ìš© ë‹¨ê³„

**ğŸŸ¡ ì¤‘ê°„ ìœ„í—˜ ìš”ì†Œë“¤**:

**Overflow ì¬í• ë‹¹ ì´ë ¥**:
```python
# run_vectorized_pipeline.py:279-281
overflow_counts = patients_df[patients_df['overflow_flag'] == True]
```
**ìœ„í—˜ì„±**: `overflow_flag`, `redistribution_method` í•„ë“œê°€ **íŠ¹ìˆ˜í•œ í™˜ìêµ° ì‹ë³„**

### 2. í•­ëª©ë³„ ì·¨ì•½ì  ë¶„ì„

#### 2.1 ì¤€ì‹ë³„ì ì¡°í•© ìœ„í—˜ë„ ë§¤íŠ¸ë¦­ìŠ¤

| í•­ëª© ì¡°í•© | ìœ„í—˜ë„ | ìœ ë‹ˆí¬ ë¹„ìœ¨ | ì¬ì‹ë³„ ë©”ì»¤ë‹ˆì¦˜ |
|----------|--------|-------------|-----------------|
| `pat_do_cd` + `emorg_cd` + `vst_dt` | ğŸ”´ **ê·¹ê³ ** | >90% | ì§€ì—­-ë³‘ì›-ë‚ ì§œ ì‚¼ì¤‘ ì§€ë¬¸ |
| `pat_age_gr` + `pat_sex` + `pat_do_cd` + `ktas_fstu` | ğŸ”´ **ê³ ìœ„í—˜** | >70% | ì¸êµ¬í†µê³„+ì„ìƒ ì¡°í•© |
| `vst_dt` + `vst_tm` + `msypt` | ğŸ”´ **ê³ ìœ„í—˜** | >80% | ì‹œê°„-ì¦ìƒ ì§€ë¬¸ |
| `emorg_cd` + `main_trt_p` + `emtrt_rust` | ğŸŸ¡ **ì¤‘ìœ„í—˜** | 30-60% | ë³‘ì›-ì§„ë£Œê³¼-ê²°ê³¼ íŒ¨í„´ |
| `pat_age_gr` + `pat_sex` + `vst_meth` | ğŸŸ¢ **ì €ìœ„í—˜** | 10-30% | ê¸°ë³¸ ì¸êµ¬í†µê³„ |

#### 2.2 í†µê³„ì  ê³µê²© ì·¨ì•½ì 

**ì°¨ë¶„ ê³µê²©(Differential Attack)**:
```python
# ë™ì¼í•œ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜ë³µ ì‚¬ìš©
cached_result = self.cache.load_cached_analysis(pattern_name, data_hash)
```
**ì·¨ì•½ì **: ìºì‹œëœ ë¶„í¬ë¥¼ ì•Œë©´ **ì—­ì¶”ë¡ ì„ í†µí•œ ê°œë³„ ë ˆì½”ë“œ ì¶”ì •** ê°€ëŠ¥

**ì—°ê²° ê³µê²©(Linkage Attack)**:
- ë³‘ì›ë³„ ì¼ì¼ ìš©ëŸ‰ (`daily_capacity_mean`)ê³¼ ì‹¤ì œ ë‚´ì›ì ìˆ˜ ë§¤ì¹­
- **ì™¸ë¶€ ë°ì´í„°ì™€ì˜ êµì°¨ ê²€ì¦ ê°€ëŠ¥ì„±**

#### 2.3 ëª¨ë¸ ì—­ì „ ê³µê²© ì·¨ì•½ì 

**íŒ¨í„´ ì¶”ë¡  ê³µê²©**:
```python
# src/analysis/pattern_analyzer.py:428-461
def get_hierarchical_ktas_distribution(region_code, hospital_type):
    # ê³„ì¸µì  KTAS ë¶„í¬ ì¡°íšŒ ë¡œì§ì´ ê³µê°œë˜ì–´ ìˆìŒ
```
**ì·¨ì•½ì **: ì•Œê³ ë¦¬ì¦˜ì´ ê³µê°œë˜ë©´ **íŠ¹ì • ì§€ì—­-ë³‘ì› ì¡°í•©ì˜ ì‹¤ì œ ë¶„í¬ ì—­ì‚°** ê°€ëŠ¥

### 3. ê°œì¸ì •ë³´ë³´í˜¸ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±

#### 3.1 ëˆ„ë½ëœ ë³´í˜¸ ê¸°ë²•ë“¤

**ì°¨ë“± í”„ë¼ì´ë²„ì‹œ(Differential Privacy) ë¯¸ì ìš©**:
- íŒ¨í„´ ë¶„ì„ ì‹œ ë…¸ì´ì¦ˆ ì£¼ì… ì—†ìŒ
- ì›ë³¸ ë¶„í¬ì™€ ê±°ì˜ ë™ì¼í•œ í•©ì„± ë¶„í¬ ìƒì„±

**k-ìµëª…ì„± ë¯¸ë³´ì¥**:
```python
# config/generation_params.yaml:58
privacy_k_anonymity: 5  # ì„¤ì •ë§Œ ìˆê³  ì‹¤ì œ ì ìš© ì•ˆë¨
```

**ì§€ë¦¬ì  ì¼ë°˜í™” ë¶€ì¡±**:
- 4ìë¦¬ ì§€ì—­ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ë³‘ì›ëª…, ì£¼ì†Œ ì •ë³´ ë³´ì¡´

---

## ğŸ’€ ì¬ì‹ë³„ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤

### 1. ì‹œë‚˜ë¦¬ì˜¤ 1: "ì§€ì—­-ë³‘ì›-ì‹œê°„ ì‚¼ì¤‘ ì§€ë¬¸ ê³µê²©"

**ğŸ¯ ê³µê²© ê°œìš”**: ê³µê²©ìê°€ íŠ¹ì • ê°œì¸ì˜ ì‘ê¸‰ì‹¤ ë‚´ì› ì‚¬ì‹¤ì„ ì•Œê³  ìˆì„ ë•Œ í•©ì„± ë°ì´í„°ì—ì„œ í•´ë‹¹ ë ˆì½”ë“œ ì‹ë³„

**ğŸ‘¤ ê³µê²©ì í”„ë¡œí•„**: 
- í”¼í•´ìì™€ ê°™ì€ ì§€ì—­ ê±°ì£¼ì (ì§€ì—­ì½”ë“œ ì•Œê³  ìˆìŒ)
- í”¼í•´ìì˜ ë³‘ì› ë‚´ì› ì‚¬ì‹¤ ëª©ê²©ì (ì†Œì…œ ë¯¸ë””ì–´, ì§€ì¸ ë“±)

**ğŸ” ê³µê²© ë‹¨ê³„**:

```python
# 1ë‹¨ê³„: í›„ë³´ ë ˆì½”ë“œ í•„í„°ë§
candidates = synthetic_data[
    (synthetic_data['pat_do_cd'] == '1101') &  # ì„œìš¸ ì¢…ë¡œêµ¬
    (synthetic_data['emorg_cd'] == 'A1234567') &  # íŠ¹ì • ëŒ€í˜•ë³‘ì›
    (synthetic_data['vst_dt'] == '20170315')  # ëª©ê²©í•œ ë‚ ì§œ
]
print(f"í›„ë³´ ë ˆì½”ë“œ ìˆ˜: {len(candidates)}")  # ì˜ˆìƒ: 5-15ê°œ

# 2ë‹¨ê³„: ì‹œê°„ëŒ€ ì¢íˆê¸° (ëª©ê²© ì‹œê°„ í™œìš©)
time_filtered = candidates[
    (candidates['vst_tm'] >= '1400') &  # ì˜¤í›„ 2ì‹œ ì´í›„
    (candidates['vst_tm'] <= '1600')   # ì˜¤í›„ 4ì‹œ ì´ì „
]
print(f"ì‹œê°„ í•„í„°ë§ í›„: {len(time_filtered)}")  # ì˜ˆìƒ: 1-3ê°œ
```

**ğŸ“Š ì„±ê³µ í™•ë¥ **: 85-95% (ì‹œê°„ ì •ë³´ ì¶”ê°€ì‹œ)

### 2. ì‹œë‚˜ë¦¬ì˜¤ 2: "í¬ê·€ íŒ¨í„´ ì‹ë³„ ê³µê²©"

**ğŸ¯ ê³µê²© ê°œìš”**: í†µê³„ì ìœ¼ë¡œ í¬ê·€í•œ íŠ¹ì„± ì¡°í•©ì„ ê°€ì§„ í™˜ì ì‹ë³„

**ğŸ” ê³µê²© ë‹¨ê³„**:

```python
# 1ë‹¨ê³„: í¬ê·€ ì¡°í•© íƒì§€
rare_combinations = synthetic_data.groupby([
    'pat_age_gr', 'pat_sex', 'pat_do_cd', 'ktas_fstu', 'msypt'
]).size().reset_index(name='count')

unique_patterns = rare_combinations[rare_combinations['count'] == 1]

# 2ë‹¨ê³„: ì„ìƒì  í¬ì†Œì„± í™œìš©
clinical_rare = synthetic_data[
    (synthetic_data['pat_age_gr'] == '90') &  # 90ì„¸ ì´ìƒ
    (synthetic_data['pat_sex'] == 'M') &      # ë‚¨ì„±
    (synthetic_data['ktas_fstu'] == '1') &    # ìµœê³  ì‘ê¸‰ë„
    (synthetic_data['msypt'].str.startswith('R57'))  # ì‡¼í¬ ì¦ìƒ
]
```

**ğŸ“Š ì„±ê³µ í™•ë¥ **: 90-95% (í¬ê·€ ì˜í•™ì  ì¡°ê±´)

### 3. ì‹œë‚˜ë¦¬ì˜¤ 3: "ìºì‹œ ê¸°ë°˜ ì°¨ë¶„ ê³µê²©"

**ğŸ¯ ê³µê²© ê°œìš”**: ìºì‹œëœ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ì´ìš©í•œ ì—­ì¶”ë¡  ê³µê²©

**ğŸ” ê³µê²© ë‹¨ê³„**:

```python
# 1ë‹¨ê³„: ìºì‹œëœ ë¶„í¬ ë¶„ì„
import pickle
with open('cache/patterns/ktas_distributions_abc123.pkl', 'rb') as f:
    cached_patterns = pickle.load(f)

# 2ë‹¨ê³„: íŠ¹ì • ì§€ì—­-ë³‘ì› ì¡°í•©ì˜ ì‹¤ì œ ë¶„í¬ ì¶”ì¶œ
target_key = "1101_large"
real_distribution = cached_patterns['detailed_patterns'][target_key]

# 3ë‹¨ê³„: í•©ì„± ë°ì´í„°ì™€ ì‹¤ì œ íŒ¨í„´ ë¹„êµë¥¼ í†µí•œ ì—­ì¶”ë¡ 
synthetic_ktas = synthetic_data.groupby(['pat_do_cd', 'hospital_type'])['ktas_fstu'].value_counts(normalize=True)
differences = real_distribution - synthetic_ktas
```

**ğŸ“Š ì„±ê³µ í™•ë¥ **: 60-100% (ìºì‹œ ì ‘ê·¼ ê°€ëŠ¥ì‹œ)

### 4. ì‹œë‚˜ë¦¬ì˜¤ 4: "ì™¸ë¶€ ë°ì´í„° ì—°ê²° ê³µê²©"

**ğŸ¯ ê³µê²© ê°œìš”**: ê³µê°œëœ ë³‘ì› ì •ë³´ì™€ í•©ì„± ë°ì´í„°ë¥¼ ë§¤ì¹­í•˜ì—¬ ì‹¤ì œ ë‚´ì› íŒ¨í„´ ì¶”ì •

**ğŸ” ê³µê²© ë‹¨ê³„**:

```python
# 1ë‹¨ê³„: ë³‘ì› ë©”íƒ€ë°ì´í„°ì™€ ë§¤ì¹­
hospital_meta = load_hospital_metadata()  # ê³µê°œ ë³‘ì› ì •ë³´
synthetic_hospitals = synthetic_data['emorg_cd'].unique()

# 2ë‹¨ê³„: ì§€ë¦¬ì  ë¶„í¬ íŒ¨í„´ ë¶„ì„  
hospital_visits = synthetic_data.groupby(['emorg_cd', 'pat_do_cd']).size()

# 3ë‹¨ê³„: ì‹¤ì œ ì¸êµ¬ ë¶„í¬ì™€ ë¹„êµ
census_data = load_population_data()
anomaly_patterns = detect_population_anomalies(synthetic_visits, census_data)
```

**ğŸ“Š ì„±ê³µ í™•ë¥ **: 40-70% (ì§€ì—­ ê·œëª¨ì— ë”°ë¼)

### 5. ì‹œë‚˜ë¦¬ì˜¤ 5: "ì•Œê³ ë¦¬ì¦˜ ì—­ì „ ê³µê²©"

**ğŸ¯ ê³µê²© ê°œìš”**: ê³µê°œëœ ìƒì„± ì•Œê³ ë¦¬ì¦˜ì„ ì—­ì‚°í•˜ì—¬ ì›ë³¸ ë°ì´í„° íŠ¹ì„± ì¶”ë¡ 

**ğŸ” ê³µê²© ë‹¨ê³„**:

```python
# 1ë‹¨ê³„: ê³„ì¸µì  ë¶„í¬ ì•Œê³ ë¦¬ì¦˜ ì—­ì‚°
def reverse_hierarchical_ktas(synthetic_records, region_code, hospital_type):
    observed_distribution = synthetic_records['ktas_fstu'].value_counts(normalize=True)
    
    # 4ë‹¨ê³„ ê³„ì¸µ ì¤‘ ì–´ëŠ ë‹¨ê³„ì—ì„œ ì˜¨ ë¶„í¬ì¸ì§€ ì¶”ì •
    hierarchy_level = estimate_hierarchy_level(observed_distribution)
    return estimate_original_distribution(observed_distribution, hierarchy_level)

# 2ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ê·œëª¨ ì¶”ì •
def estimate_original_sample_size(synthetic_data, region_code):
    regional_records = synthetic_data[synthetic_data['pat_do_cd'] == region_code]
    # min_sample_size >= 10 ì¡°ê±´ í™œìš©
    estimated_original = estimate_from_synthetic_size(len(regional_records), 10)
    return estimated_original
```

**ğŸ“Š ì„±ê³µ í™•ë¥ **: 60-90% (ì•Œê³ ë¦¬ì¦˜ ê³µê°œë¡œ ì¸í•œ ì—­ì‚°)

---

## ğŸ›¡ï¸ ëŒ€ì‘ ë°©ì•ˆ ë° ê°œì„  ê¶Œê³ ì‚¬í•­

### 1. ì¦‰ì‹œ ì ìš© ê¶Œê³ ì‚¬í•­ (Tier 1: ê¸´ê¸‰ ì¡°ì¹˜)

#### 1.1 ì§€ë¦¬ì  ì¼ë°˜í™” ê°•í™”

```python
def generalize_region_code(region_code: str) -> str:
    """4ìë¦¬ â†’ 2ìë¦¬ ëŒ€ë¶„ë¥˜ë¡œ ì¼ë°˜í™”"""
    return region_code[:2] if len(region_code) >= 2 else region_code

def apply_geographic_generalization(data: pd.DataFrame) -> pd.DataFrame:
    """ì§€ë¦¬ì  ì¼ë°˜í™” ì ìš©"""
    data = data.copy()
    data['pat_do_cd_generalized'] = data['pat_do_cd'].apply(generalize_region_code)
    data = data.drop(['pat_do_cd'], axis=1)  # ì›ë³¸ ì œê±°
    return data
```

#### 1.2 ì‹œê°„ í•´ìƒë„ ê°ì†Œ

```python
def reduce_temporal_precision(vst_dt: str, vst_tm: str) -> Tuple[str, str]:
    """ì‹œê°„ì„ 4ì‹œê°„ ë‹¨ìœ„ë¡œ ì¼ë°˜í™”"""
    hour = int(vst_tm[:2])
    generalized_hour = (hour // 4) * 4
    
    # ì£¼ ë‹¨ìœ„ë¡œ ë‚ ì§œ ì¼ë°˜í™” ì˜µì…˜
    date_obj = datetime.strptime(vst_dt, '%Y%m%d')
    week_start = date_obj - timedelta(days=date_obj.weekday())
    generalized_date = week_start.strftime('%Y%m%d')
    
    return generalized_date, f"{generalized_hour:02d}00"
```

#### 1.3 ìºì‹œ ì•”í˜¸í™”

```python
from cryptography.fernet import Fernet

class EncryptedAnalysisCache(AnalysisCache):
    def __init__(self, cache_dir: str, encryption_key: bytes):
        super().__init__(cache_dir)
        self.cipher = Fernet(encryption_key)
    
    def save_analysis_cache(self, analysis_type: str, data_hash: str, results: Dict[str, Any]):
        """ì•”í˜¸í™”ëœ ìºì‹œ ì €ì¥"""
        cache_key = f"{analysis_type}_{data_hash}"
        cache_file = self.cache_dir / f"{cache_key}.encrypted"
        
        # ë°ì´í„° ì•”í˜¸í™”
        serialized_data = pickle.dumps(results)
        encrypted_data = self.cipher.encrypt(serialized_data)
        
        with open(cache_file, 'wb') as f:
            f.write(encrypted_data)
```

### 2. ì¤‘ê¸° ê°œì„ ì‚¬í•­ (Tier 2: 3-6ê°œì›”)

#### 2.1 ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ë„ì…

```python
import numpy as np

def add_differential_privacy_noise(distribution: Dict[str, float], 
                                 epsilon: float = 1.0) -> Dict[str, float]:
    """ë¶„í¬ì— ë¼í”Œë¼ìŠ¤ ë…¸ì´ì¦ˆ ì¶”ê°€"""
    noise_scale = 1.0 / epsilon
    
    noisy_distribution = {}
    for key, prob in distribution.items():
        # ë¼í”Œë¼ìŠ¤ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.laplace(0, noise_scale / len(distribution))
        noisy_prob = max(0, prob + noise)  # ìŒìˆ˜ ë°©ì§€
        noisy_distribution[key] = noisy_prob
    
    # ì •ê·œí™”
    total = sum(noisy_distribution.values())
    return {k: v/total for k, v in noisy_distribution.items()}

class PrivacyAwarePatternAnalyzer(PatternAnalyzer):
    def __init__(self, db_manager, config, privacy_budget: float = 10.0):
        super().__init__(db_manager, config)
        self.privacy_budget = privacy_budget
        self.privacy_used = 0.0
    
    def analyze_ktas_distributions_with_privacy(self) -> Dict[str, Any]:
        """ì°¨ë“± í”„ë¼ì´ë²„ì‹œê°€ ì ìš©ëœ KTAS ë¶„í¬ ë¶„ì„"""
        # ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
        base_analysis = super().analyze_ktas_distributions()
        
        # ê° íŒ¨í„´ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        epsilon_per_pattern = self.privacy_budget / 4  # 4ë‹¨ê³„ ê³„ì¸µ
        
        for pattern_type in ['detailed_patterns', 'major_patterns', 'national_patterns']:
            if pattern_type in base_analysis:
                for key, distribution in base_analysis[pattern_type].items():
                    prob_dict = {k: v['probability'] for k, v in distribution.items()}
                    noisy_probs = add_differential_privacy_noise(prob_dict, epsilon_per_pattern)
                    
                    # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í™•ë¥ ë¡œ ì—…ë°ì´íŠ¸
                    for k in distribution:
                        distribution[k]['probability'] = noisy_probs[k]
        
        self.privacy_used += self.privacy_budget
        return base_analysis
```

#### 2.2 k-ìµëª…ì„± ë³´ì¥

```python
def ensure_k_anonymity(data: pd.DataFrame, 
                      quasi_identifiers: List[str], 
                      k: int = 5) -> pd.DataFrame:
    """k-ìµëª…ì„± ì¡°ê±´ í™•ì¸ ë° ì¡°ì •"""
    
    # ì¤€ì‹ë³„ì ì¡°í•©ë³„ ë¹ˆë„ ê³„ì‚°
    group_counts = data.groupby(quasi_identifiers).size()
    violating_groups = group_counts[group_counts < k]
    
    if len(violating_groups) == 0:
        return data
    
    logger = logging.getLogger(__name__)
    logger.warning(f"Found {len(violating_groups)} groups violating k-anonymity (k={k})")
    
    # ìœ„ë°˜ ê·¸ë£¹ ì²˜ë¦¬ - ì¼ë°˜í™” ë˜ëŠ” ì–µì œ
    processed_data = data.copy()
    
    for group_values in violating_groups.index:
        # í•´ë‹¹ ê·¸ë£¹ ë ˆì½”ë“œ ì‹ë³„
        mask = (data[quasi_identifiers] == group_values).all(axis=1)
        violating_records = data[mask]
        
        # ì–µì œ ë°©ì‹: ìœ„ë°˜ ë ˆì½”ë“œ ì œê±°
        processed_data = processed_data[~mask]
        
        # ë˜ëŠ” ì¼ë°˜í™” ë°©ì‹: ë” ìƒìœ„ ë²”ì£¼ë¡œ ì¼ë°˜í™”
        # processed_data = generalize_violating_records(processed_data, violating_records, quasi_identifiers)
    
    logger.info(f"k-anonymity processing: {len(data)} â†’ {len(processed_data)} records")
    return processed_data

def apply_k_anonymity_to_pipeline(patients_df: pd.DataFrame) -> pd.DataFrame:
    """íŒŒì´í”„ë¼ì¸ì— k-ìµëª…ì„± ì ìš©"""
    quasi_identifiers = [
        'pat_age_gr_generalized',  # ì¼ë°˜í™”ëœ ì—°ë ¹ê·¸ë£¹
        'pat_sex',
        'pat_do_cd_major',         # ì¼ë°˜í™”ëœ ì§€ì—­ì½”ë“œ (2ìë¦¬)
        'hospital_type'            # ë³‘ì› ìœ í˜•
    ]
    
    return ensure_k_anonymity(patients_df, quasi_identifiers, k=10)
```

#### 2.3 ì ì‘ì  ë…¸ì´ì¦ˆ ì£¼ì…

```python
def adaptive_noise_injection(pattern_type: str, 
                           data_size: int, 
                           privacy_level: float) -> float:
    """ë°ì´í„° í¬ê¸°ì™€ í”„ë¼ì´ë²„ì‹œ ìˆ˜ì¤€ì— ë”°ë¥¸ ì ì‘ì  ë…¸ì´ì¦ˆ"""
    
    base_epsilon = privacy_level
    
    # íŒ¨í„´ë³„ ë¯¼ê°ë„ ì¡°ì •
    sensitivity_multiplier = {
        'rare_pattern': 0.5,      # í¬ê·€ íŒ¨í„´ì€ ë” ë§ì€ ë…¸ì´ì¦ˆ
        'common_pattern': 1.5,    # ì¼ë°˜ íŒ¨í„´ì€ ì ì€ ë…¸ì´ì¦ˆ
        'geographic_pattern': 0.3, # ì§€ë¦¬ì  íŒ¨í„´ì€ ê°•í•œ ë³´í˜¸
        'temporal_pattern': 0.8,   # ì‹œê°„ íŒ¨í„´ì€ ì¤‘ê°„ ë³´í˜¸
        'clinical_pattern': 0.4    # ì„ìƒ íŒ¨í„´ì€ ê°•í•œ ë³´í˜¸
    }
    
    # ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì¡°ì •
    size_adjustment = min(1.0, np.log(data_size + 1) / 10)
    
    adjusted_epsilon = base_epsilon * sensitivity_multiplier.get(pattern_type, 1.0) * size_adjustment
    
    return adjusted_epsilon

class AdaptivePrivacyPatternAnalyzer(PatternAnalyzer):
    def analyze_pattern_with_adaptive_privacy(self, pattern_type: str, 
                                            base_data: pd.DataFrame) -> Dict[str, Any]:
        """ì ì‘ì  í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ë¥¼ ì ìš©í•œ íŒ¨í„´ ë¶„ì„"""
        
        # ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
        base_analysis = self._perform_base_analysis(pattern_type, base_data)
        
        # ê° íŒ¨í„´ë³„ ì ì‘ì  ë…¸ì´ì¦ˆ ê³„ì‚°
        protected_analysis = {}
        
        for key, pattern_data in base_analysis.items():
            data_size = pattern_data.get('sample_size', len(base_data))
            epsilon = adaptive_noise_injection(pattern_type, data_size, self.privacy_level)
            
            # ì ì‘ì  ë…¸ì´ì¦ˆ ì¶”ê°€
            if 'distribution' in pattern_data:
                noisy_distribution = add_differential_privacy_noise(
                    pattern_data['distribution'], epsilon
                )
                protected_analysis[key] = {
                    **pattern_data,
                    'distribution': noisy_distribution,
                    'privacy_epsilon': epsilon,
                    'privacy_applied': True
                }
            else:
                protected_analysis[key] = pattern_data
        
        return protected_analysis
```

### 3. ì¥ê¸° ì‹œìŠ¤í…œ ê°œì„  (Tier 3: 6-12ê°œì›”)

#### 3.1 í•©ì„± ë°ì´í„° í’ˆì§ˆ ìµœì í™”

```python
class PrivacyUtilityOptimizer:
    """í”„ë¼ì´ë²„ì‹œ-ìœ ìš©ì„± ê· í˜•ì  ìµœì í™”"""
    
    def __init__(self, privacy_budget: float = 1.0, utility_threshold: float = 0.8):
        self.privacy_budget = privacy_budget
        self.utility_threshold = utility_threshold
        self.pareto_front = []
    
    def calculate_privacy_loss(self, original_data: pd.DataFrame, 
                             synthetic_data: pd.DataFrame) -> float:
        """í”„ë¼ì´ë²„ì‹œ ì†ì‹¤ ê³„ì‚°"""
        # ì¬ì‹ë³„ ìœ„í—˜ë„ ê¸°ë°˜ í”„ë¼ì´ë²„ì‹œ ì†ì‹¤ ì¸¡ì •
        risk_scores = []
        
        # ì¤€ì‹ë³„ì ì¡°í•©ë³„ ìœ ë‹ˆí¬ì„± ì¸¡ì •
        quasi_identifiers = ['pat_do_cd_major', 'pat_age_gr', 'pat_sex', 'hospital_type']
        
        for qi_combo in itertools.combinations(quasi_identifiers, 3):
            orig_unique_ratio = self._calculate_uniqueness_ratio(original_data, qi_combo)
            synth_unique_ratio = self._calculate_uniqueness_ratio(synthetic_data, qi_combo)
            
            # ìœ ë‹ˆí¬ì„± ë¹„ìœ¨ ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ì¬ì‹ë³„ ìœ„í—˜ ë†’ìŒ
            risk_score = 1.0 - abs(orig_unique_ratio - synth_unique_ratio)
            risk_scores.append(risk_score)
        
        return np.mean(risk_scores)
    
    def calculate_utility_score(self, original_data: pd.DataFrame, 
                              synthetic_data: pd.DataFrame) -> float:
        """ë°ì´í„° ìœ ìš©ì„± ì ìˆ˜ ê³„ì‚°"""
        utility_scores = []
        
        # í†µê³„ì  ìœ ì‚¬ì„± ì¸¡ì •
        for column in original_data.select_dtypes(include=[np.number]).columns:
            # Kolmogorov-Smirnov ê²€ì •
            ks_stat, ks_pvalue = stats.ks_2samp(
                original_data[column].dropna(), 
                synthetic_data[column].dropna()
            )
            utility_scores.append(1.0 - ks_stat)  # ë‚®ì€ KS í†µê³„ëŸ‰ = ë†’ì€ ìœ ì‚¬ì„±
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ Chi-square ê²€ì •
        for column in original_data.select_dtypes(include=['object', 'category']).columns:
            orig_dist = original_data[column].value_counts(normalize=True)
            synth_dist = synthetic_data[column].value_counts(normalize=True)
            
            # ë¶„í¬ ê°„ ê±°ë¦¬ ê³„ì‚°
            common_categories = set(orig_dist.index) & set(synth_dist.index)
            if common_categories:
                orig_common = orig_dist[list(common_categories)]
                synth_common = synth_dist[list(common_categories)]
                
                # Wasserstein distance ê³„ì‚°
                distance = wasserstein_distance(orig_common.values, synth_common.values)
                utility_scores.append(1.0 - min(distance, 1.0))
        
        return np.mean(utility_scores)
    
    def optimize_parameters(self, original_data: pd.DataFrame) -> Dict[str, float]:
        """ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰"""
        
        best_params = None
        best_score = -1
        
        # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ íƒìƒ‰
        epsilon_values = [0.1, 0.5, 1.0, 2.0, 5.0]
        k_values = [5, 10, 15, 20]
        generalization_levels = ['low', 'medium', 'high']
        
        for epsilon in epsilon_values:
            for k in k_values:
                for gen_level in generalization_levels:
                    # íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í•©ì„± ë°ì´í„° ìƒì„±
                    synthetic_data = self._generate_synthetic_with_params(
                        original_data, epsilon, k, gen_level
                    )
                    
                    # í”„ë¼ì´ë²„ì‹œ-ìœ ìš©ì„± ì ìˆ˜ ê³„ì‚°
                    privacy_score = 1.0 - self.calculate_privacy_loss(original_data, synthetic_data)
                    utility_score = self.calculate_utility_score(original_data, synthetic_data)
                    
                    # ìœ ìš©ì„± ì„ê³„ê°’ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ ê³ ë ¤
                    if utility_score >= self.utility_threshold:
                        combined_score = privacy_score * 0.6 + utility_score * 0.4
                        
                        if combined_score > best_score:
                            best_score = combined_score
                            best_params = {
                                'epsilon': epsilon,
                                'k_anonymity': k,
                                'generalization_level': gen_level,
                                'privacy_score': privacy_score,
                                'utility_score': utility_score,
                                'combined_score': combined_score
                            }
        
        return best_params
```

#### 3.2 ì‹¤ì‹œê°„ ìœ„í—˜ ëª¨ë‹ˆí„°ë§

```python
class ReidentificationRiskMonitor:
    """ì‹¤ì‹œê°„ ì¬ì‹ë³„ ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, risk_threshold: float = 0.05):
        self.risk_threshold = risk_threshold
        self.risk_history = []
        self.alerts = []
    
    def monitor_data_release(self, synthetic_data: pd.DataFrame, 
                           original_data: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ê³µê°œ ì‹œ ì‹¤ì‹œê°„ ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        
        risk_assessment = {
            'timestamp': datetime.now().isoformat(),
            'total_records': len(synthetic_data),
            'risk_scores': {},
            'recommendations': [],
            'overall_risk': 'LOW'
        }
        
        # 1. ìœ ë‹ˆí¬ì„± ìœ„í—˜ ì¸¡ì •
        uniqueness_risk = self._assess_uniqueness_risk(synthetic_data)
        risk_assessment['risk_scores']['uniqueness'] = uniqueness_risk
        
        # 2. í¬ê·€ íŒ¨í„´ ìœ„í—˜ ì¸¡ì •
        rare_pattern_risk = self._assess_rare_pattern_risk(synthetic_data)
        risk_assessment['risk_scores']['rare_patterns'] = rare_pattern_risk
        
        # 3. ì—°ê²° ê³µê²© ìœ„í—˜ ì¸¡ì •
        linkage_risk = self._assess_linkage_risk(synthetic_data, original_data)
        risk_assessment['risk_scores']['linkage_attack'] = linkage_risk
        
        # 4. ì°¨ë¶„ ê³µê²© ìœ„í—˜ ì¸¡ì •
        differential_risk = self._assess_differential_attack_risk(synthetic_data)
        risk_assessment['risk_scores']['differential_attack'] = differential_risk
        
        # 5. ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°
        overall_risk_score = np.mean([
            uniqueness_risk, rare_pattern_risk, linkage_risk, differential_risk
        ])
        
        if overall_risk_score > 0.8:
            risk_assessment['overall_risk'] = 'HIGH'
            risk_assessment['recommendations'].append('ì¦‰ì‹œ ë°ì´í„° ê³µê°œ ì¤‘ë‹¨ í•„ìš”')
        elif overall_risk_score > 0.5:
            risk_assessment['overall_risk'] = 'MEDIUM'
            risk_assessment['recommendations'].append('ì¶”ê°€ ë³´í˜¸ ì¡°ì¹˜ ì ìš© ê¶Œì¥')
        else:
            risk_assessment['overall_risk'] = 'LOW'
        
        # ìœ„í—˜ ì´ë ¥ ì €ì¥
        self.risk_history.append(risk_assessment)
        
        # ì•Œë¦¼ ìƒì„±
        if overall_risk_score > self.risk_threshold:
            self._generate_alert(risk_assessment)
        
        return risk_assessment
    
    def _assess_uniqueness_risk(self, data: pd.DataFrame) -> float:
        """ìœ ë‹ˆí¬ì„± ê¸°ë°˜ ìœ„í—˜ í‰ê°€"""
        quasi_identifiers = ['pat_do_cd_major', 'pat_age_gr', 'pat_sex', 'hospital_type']
        
        unique_combinations = []
        for r in range(2, len(quasi_identifiers) + 1):
            for qi_combo in itertools.combinations(quasi_identifiers, r):
                group_sizes = data.groupby(list(qi_combo)).size()
                unique_ratio = (group_sizes == 1).sum() / len(group_sizes)
                unique_combinations.append(unique_ratio)
        
        return np.max(unique_combinations)  # ìµœëŒ€ ìœ ë‹ˆí¬ì„± ë¹„ìœ¨
    
    def _generate_alert(self, risk_assessment: Dict[str, Any]):
        """ìœ„í—˜ ì•Œë¦¼ ìƒì„±"""
        alert = {
            'timestamp': risk_assessment['timestamp'],
            'risk_level': risk_assessment['overall_risk'],
            'risk_scores': risk_assessment['risk_scores'],
            'action_required': True,
            'recommendations': risk_assessment['recommendations']
        }
        
        self.alerts.append(alert)
        
        # ë¡œê¹…
        logger = logging.getLogger(__name__)
        logger.warning(f"HIGH RISK DETECTED: {alert}")
        
        # ì™¸ë¶€ ì•Œë¦¼ ì‹œìŠ¤í…œ ì—°ë™ ê°€ëŠ¥ (ì´ë©”ì¼, Slack ë“±)
```

### 4. ê¶Œì¥ ì„¤ì •ê°’ ë° ì •ì±…

#### 4.1 í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ íŒŒë¼ë¯¸í„°

```yaml
# config/privacy_protection.yaml
privacy_protection:
  # ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ì„¤ì •
  differential_privacy:
    global_epsilon: 5.0      # ì „ì²´ í”„ë¼ì´ë²„ì‹œ ì˜ˆì‚°
    delta: 1e-5              # í”„ë¼ì´ë²„ì‹œ ì‹¤íŒ¨ í™•ë¥ 
    composition: "advanced"   # ê³ ê¸‰ í•©ì„± ì •ë¦¬ ì‚¬ìš©
    
  # k-ìµëª…ì„± ì„¤ì •
  k_anonymity:
    k_value: 10              # ìµœì†Œ ê·¸ë£¹ í¬ê¸°
    quasi_identifiers:
      - pat_age_gr_generalized  # 10ì„¸ ë‹¨ìœ„ ì¼ë°˜í™”
      - pat_sex
      - pat_do_cd_major        # 2ìë¦¬ ëŒ€ë¶„ë¥˜
      - hospital_type_generalized
    
  # ì‹œê°„ í”„ë¼ì´ë²„ì‹œ ì„¤ì •
  temporal_privacy:
    date_resolution: "week"   # ì£¼ ë‹¨ìœ„ ì¼ë°˜í™”
    time_resolution: "4hour"  # 4ì‹œê°„ ë¸”ë¡
    holiday_generalization: true  # ê³µíœ´ì¼ ì¼ë°˜í™”
    
  # ì§€ë¦¬ì  í”„ë¼ì´ë²„ì‹œ ì„¤ì •
  geographic_privacy:
    region_level: "major"     # ëŒ€ë¶„ë¥˜(2ìë¦¬)ë§Œ ì‚¬ìš©
    hospital_anonymization: true
    distance_threshold: 50    # 50km ì´ìƒ ê±°ë¦¬ëŠ” ë™ì¼ ì²˜ë¦¬
    
  # ì„ìƒ ë°ì´í„° ë³´í˜¸
  clinical_privacy:
    rare_condition_threshold: 10  # 10ê±´ ë¯¸ë§Œ ì§ˆí™˜ ì¼ë°˜í™”
    diagnosis_generalization: 3   # 3ìë¦¬ê¹Œì§€ë§Œ ì‚¬ìš©
    vital_sign_binning: true      # ë°”ì´íƒˆ ì‚¬ì¸ êµ¬ê°„í™”
```

#### 4.2 ë°ì´í„° í’ˆì§ˆ vs í”„ë¼ì´ë²„ì‹œ ê· í˜•ì 

```python
# ê¶Œì¥ ì„¤ì •ì— ë”°ë¥¸ ì˜ˆìƒ íš¨ê³¼
RECOMMENDED_SETTINGS = {
    'privacy_protection': {
        'epsilon': 3.0,           # ì ì ˆí•œ í”„ë¼ì´ë²„ì‹œ-ìœ ìš©ì„± ê· í˜•
        'k_anonymity': 10,        # ì¶©ë¶„í•œ ìµëª…ì„± ë³´ì¥
        'geographic_generalization': 2,  # 2ìë¦¬ ì§€ì—­ì½”ë“œ
        'temporal_resolution': '4hour',  # 4ì‹œê°„ ë¸”ë¡
    },
    'expected_outcomes': {
        'reidentification_risk_reduction': '95% â†’ 8%',  # ì¬ì‹ë³„ ìœ„í—˜ ëŒ€í­ ê°ì†Œ
        'data_utility_retention': '85%',               # ìœ ìš©ì„± 85% ìœ ì§€  
        'performance_impact': '<5%',                    # ì„±ëŠ¥ ì˜í–¥ ë¯¸ë¯¸
        'statistical_validity': 'maintained',          # í†µê³„ì  ìœ íš¨ì„± ìœ ì§€
    }
}
```

---

## ğŸ“ˆ ìµœì¢… í‰ê°€ ë° ê²°ë¡ 

### 1. ì¢…í•© í‰ê°€ ìš”ì•½

#### 1.1 ê¸°ìˆ ì  ì„±ê³¼ âœ…

**í˜ì‹ ì  ì•„í‚¤í…ì²˜**:
- **50ë°° ì„±ëŠ¥ í–¥ìƒ** (300ì´ˆ â†’ 7ì´ˆ) ë‹¬ì„±
- **ë™ì  íŒ¨í„´ í•™ìŠµ** ì‹œìŠ¤í…œìœ¼ë¡œ í•˜ë“œì½”ë”© ì™„ì „ ì œê±°
- **ê³„ì¸µì  ëŒ€ì•ˆ ì‹œìŠ¤í…œ** êµ¬í˜„ìœ¼ë¡œ ë°ì´í„° í¬ì†Œì„± ë¬¸ì œ í•´ê²°
- **3-Stage ë²¡í„°í™” íŒŒì´í”„ë¼ì¸** ì™„ì„±

**í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±**:
- ì „êµ­ ê·œëª¨ (17ê°œ ì‹œë„, 460ê°œ ì´ìƒ ë³‘ì›) í™•ì¥ ê°€ëŠ¥
- ìºì‹± ì‹œìŠ¤í…œì„ í†µí•œ íš¨ìœ¨ì ì¸ ì¬ë¶„ì„
- ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ê°œë³„ ì»´í¬ë„ŒíŠ¸ êµì²´ ê°€ëŠ¥

#### 1.2 ê°œì¸ì •ë³´ë³´í˜¸ ìœ„í—˜ë„ ğŸš¨

**í˜„ì¬ ìœ„í—˜ ìˆ˜ì¤€**: **HIGH (ê³ ìœ„í—˜)**

| ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ | ì„±ê³µ í™•ë¥  | ì‹¬ê°ë„ | ëŒ€ìƒ ê·œëª¨ |
|-------------|----------|--------|----------|
| ì§€ì—­-ë³‘ì›-ì‹œê°„ ì‚¼ì¤‘ ì§€ë¬¸ | 85-95% | ğŸ”´ ê·¹ê³  | ê°œë³„ í™˜ì |
| í¬ê·€ íŒ¨í„´ ì‹ë³„ | 90-95% | ğŸ”´ ê³  | í¬ê·€ì§ˆí™˜ì |
| ìºì‹œ ê¸°ë°˜ ì°¨ë¶„ ê³µê²© | 60-100% | ğŸŸ¡ ì¤‘ | ì§€ì—­ ë‹¨ìœ„ |
| ì™¸ë¶€ ë°ì´í„° ì—°ê²° | 40-70% | ğŸŸ¡ ì¤‘ | ì§€ì—­ ë‹¨ìœ„ |
| ì•Œê³ ë¦¬ì¦˜ ì—­ì „ | 60-90% | ğŸŸ¡ ì¤‘ | ì „ì²´ |

#### 1.3 ì£¼ìš” ì·¨ì•½ì 

1. **ê³¼ë„í•œ ì„¸ë¶€ì„±**: 4ìë¦¬ ì§€ì—­ì½”ë“œ, ì •í™•í•œ ì‹œê°„, íŠ¹ì • ë³‘ì› ì‹ë³„ ì •ë³´
2. **íŒ¨í„´ ì™„ì „ ë³´ì¡´**: ì›ë³¸ê³¼ ê±°ì˜ ë™ì¼í•œ ë¶„í¬ë¡œ ì¸í•œ ì—­ì¶”ë¡  ìœ„í—˜
3. **ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±**: ì°¨ë“± í”„ë¼ì´ë²„ì‹œ, k-ìµëª…ì„± ë“± ê¸°ë³¸ ë³´í˜¸ ê¸°ë²• ë¯¸ì ìš©
4. **ìºì‹œ ë³´ì•ˆ ì·¨ì•½**: ì›ë³¸ ë¶„í¬ ì •ë³´ê°€ í‰ë¬¸ìœ¼ë¡œ ì €ì¥
5. **íˆ¬ëª…í•œ ì•Œê³ ë¦¬ì¦˜**: ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì¸í•œ ì—­ì‚° ê³µê²© ê°€ëŠ¥ì„±

### 2. ë‹¨ê³„ë³„ ê°œì„  ë¡œë“œë§µ

#### 2.1 Phase 1: ì¦‰ì‹œ ì¡°ì¹˜ (1-2ê°œì›”) ğŸš¨

**í•„ìˆ˜ ë³´ì•ˆ ì¡°ì¹˜**:
```python
# 1. ì§€ë¦¬ì  ì¼ë°˜í™” (4ìë¦¬ â†’ 2ìë¦¬)
pat_do_cd_major = pat_do_cd[:2]

# 2. ì‹œê°„ í•´ìƒë„ ê°ì†Œ (1ì‹œê°„ â†’ 4ì‹œê°„ ë¸”ë¡)
time_block = (hour // 4) * 4

# 3. ìºì‹œ ì•”í˜¸í™”
encrypted_cache = encrypt_with_key(analysis_cache, secret_key)

# 4. í¬ê·€ íŒ¨í„´ ì–µì œ (ë¹ˆë„ < 10ì¸ ì¡°í•© ì œê±°)
filtered_data = data.groupby(quasi_identifiers).filter(lambda x: len(x) >= 10)
```

**ì˜ˆìƒ íš¨ê³¼**: ì¬ì‹ë³„ ìœ„í—˜ 85% â†’ 25% ê°ì†Œ

#### 2.2 Phase 2: ì¤‘ê¸° ê°•í™” (3-6ê°œì›”) ğŸ›¡ï¸

**í”„ë¼ì´ë²„ì‹œ ë©”ì»¤ë‹ˆì¦˜ ë„ì…**:
```python
# ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ì ìš© (Îµ=3.0)
noisy_distribution = add_laplace_noise(original_distribution, epsilon=3.0)

# k-ìµëª…ì„± ë³´ì¥ (k=10)
k_anonymous_data = ensure_k_anonymity(data, quasi_identifiers, k=10)

# ì ì‘ì  ë…¸ì´ì¦ˆ ì£¼ì…
adaptive_noise = calculate_adaptive_noise(pattern_type, data_size)
```

**ì˜ˆìƒ íš¨ê³¼**: ì¬ì‹ë³„ ìœ„í—˜ 25% â†’ 8% ê°ì†Œ

#### 2.3 Phase 3: ì¥ê¸° ìµœì í™” (6-12ê°œì›”) âš™ï¸

**ê³ ê¸‰ ë³´í˜¸ ê¸°ë²•**:
- í•©ì„± ë°ì´í„° ì „ìš© í”„ë¼ì´ë²„ì‹œ ë©”íŠ¸ë¦­ ê°œë°œ
- ì‹¤ì‹œê°„ ì¬ì‹ë³„ ìœ„í—˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- í”„ë¼ì´ë²„ì‹œ-ìœ ìš©ì„± ìµœì í™” ìë™í™”

**ì˜ˆìƒ íš¨ê³¼**: ì¬ì‹ë³„ ìœ„í—˜ 8% â†’ 3% ì´í•˜ ë‹¬ì„±

### 3. ìµœì¢… ê¶Œê³ ì‚¬í•­

#### 3.1 ì¦‰ì‹œ ì‹¤í–‰ í•„ìš” âš ï¸

**í˜„ì¬ ìƒíƒœë¡œëŠ” ì‹¤í™˜ê²½ ë°°í¬ ë¶€ì ì ˆ**. ë‹¤ìŒ ì¡°ì¹˜ í›„ ì œí•œì  í™œìš© ê¶Œì¥:

1. **ì§€ì—­ì½”ë“œ ì¼ë°˜í™”**: 4ìë¦¬ â†’ 2ìë¦¬ (ì‹œë„ ë‹¨ìœ„)
2. **ì‹œê°„ í•´ìƒë„ ê°ì†Œ**: ì‹œê°„ë³„ â†’ 4ì‹œê°„ ë¸”ë¡
3. **ìºì‹œ ë³´ì•ˆ ê°•í™”**: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì•”í˜¸í™”
4. **í¬ê·€ íŒ¨í„´ ì–µì œ**: ë¹ˆë„ 10 ë¯¸ë§Œ ì¡°í•© ì¼ë°˜í™”

#### 3.2 ì¥ê¸°ì  ëª©í‘œ ğŸ¯

**ì„¸ê³„ ìˆ˜ì¤€ì˜ í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ í•©ì„± ë°ì´í„° ì‹œìŠ¤í…œ** êµ¬ì¶•:

- **ì°¨ë“± í”„ë¼ì´ë²„ì‹œ í‘œì¤€**: IEEE 2857-2021 ì¤€ìˆ˜
- **k-ìµëª…ì„± ë³´ì¥**: GDPR Article 25 ìš”êµ¬ì‚¬í•­ ë§Œì¡±  
- **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ISO/IEC 27001 ë³´ì•ˆ ê´€ë¦¬ ì²´ê³„
- **íˆ¬ëª…ì„± vs ë³´ì•ˆ**: ì˜¤í”ˆì†ŒìŠ¤ì˜ ì¥ì ì„ ìœ ì§€í•˜ë©´ì„œ ë³´ì•ˆ ê°•í™”

#### 3.3 ê¸°ëŒ€ íš¨ê³¼ ğŸ“Š

**ë³´ì•ˆ ê°•í™” í›„ ì˜ˆìƒ ê²°ê³¼**:
```
ì¬ì‹ë³„ ìœ„í—˜:     95% â†’ 5% ì´í•˜
ë°ì´í„° ìœ ìš©ì„±:   í˜„ì¬ ìˆ˜ì¤€ì˜ 85% ìœ ì§€
ì²˜ë¦¬ ì„±ëŠ¥:       í˜„ì¬ ëŒ€ë¹„ 95% ìœ ì§€ (5% ë‚´ ì˜í–¥)
ë²•ì  ì¤€ìˆ˜ì„±:     GDPR, HIPAA, ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜
```

**ì‚¬íšŒì  ê°€ì¹˜**:
- ì˜ë£Œ ë°ì´í„° ì—°êµ¬ í™œì„±í™”
- ê°œì¸ì •ë³´ ê±±ì • ì—†ëŠ” AI ëª¨ë¸ í•™ìŠµ í™˜ê²½ ì œê³µ
- êµ­ê°€ ì‘ê¸‰ì˜ë£Œ ì •ì±… ìˆ˜ë¦½ ì§€ì›
- ê¸€ë¡œë²Œ í‘œì¤€ í•©ì„± ë°ì´í„° ì‹œìŠ¤í…œ ì„ ë„

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ê´€ë ¨ ë¬¸ì„œ
- [ë²¡í„°í™” ìƒì„± ì•Œê³ ë¦¬ì¦˜ ë¬¸ì„œ](docs/vectorized_generation_algorithm.md)
- [ê°œë°œ ê°€ì´ë“œë¼ì¸](CLAUDE.md)
- [ì„¤ì • íŒŒì¼](config/generation_params.yaml)

### í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
- [íŒ¨í„´ ë¶„ì„ê¸°](src/analysis/pattern_analyzer.py)
- [ë²¡í„°í™” í™˜ì ìƒì„±ê¸°](src/vectorized/patient_generator.py)
- [ì‹œê°„ íŒ¨í„´ í• ë‹¹ê¸°](src/vectorized/temporal_assigner.py)
- [ìš©ëŸ‰ ì œì•½ ì²˜ë¦¬ê¸°](src/vectorized/capacity_processor.py)
- [ë©”ì¸ íŒŒì´í”„ë¼ì¸](scripts/run_vectorized_pipeline.py)

### í”„ë¼ì´ë²„ì‹œ ê´€ë ¨ í‘œì¤€
- IEEE 2857-2021: Privacy Engineering for System Life Cycle Processes
- ISO/IEC 27001: Information Security Management Systems
- NIST Privacy Framework 1.0
- GDPR Article 25: Data Protection by Design and by Default

---

*ë³¸ ë¶„ì„ì€ 2025ë…„ 1ì›” ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.*